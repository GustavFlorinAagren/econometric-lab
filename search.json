[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Econometrics Lab",
    "section": "",
    "text": "1 Welcome!\nWelcome to my econometrics lab.\nThis is a portfolio of my work in econometrics. The site will be updated as I continue to learn and grow in the field of econometrics. As of now, the site contains the following sections:",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome!</span>"
    ]
  },
  {
    "objectID": "index.html#contents",
    "href": "index.html#contents",
    "title": "Econometrics Lab",
    "section": "1.1 Contents",
    "text": "1.1 Contents\n\nOld School Econometrics\nModern Econometrics\nMachine Learning\n\n\nThis lab is licensed under a Creative Commons Attribution 4.0 International License.",
    "crumbs": [
      "<span class='chapter-number'>1</span>  <span class='chapter-title'>Welcome!</span>"
    ]
  },
  {
    "objectID": "Old-school-econometrics.html",
    "href": "Old-school-econometrics.html",
    "title": "Old-school econometrics",
    "section": "",
    "text": "This part will cover classical econometrical methods and solutions to different problems. The topics covered in this section are:\n\nLinear regression\nLogit and probit\nSurvival analysis\nSelection correction",
    "crumbs": [
      "Old-school econometrics"
    ]
  },
  {
    "objectID": "01-Linear-regression.html",
    "href": "01-Linear-regression.html",
    "title": "2  Linear regression",
    "section": "",
    "text": "2.1 The boston data set\nThis section will make use of the data set Boston. It contains information collected by the U.S. Census Service concerning housing and crime rates in Boston (ref: https://www.cs.toronto.edu/~delve/data/boston/bostonDetail.html).\nThe variables in the data set are:\n# Print the table using gt\nvariable_descriptions %&gt;% \n  gt() %&gt;% \n  tab_header(\"Variables\") \n\n\n\n\n\n\n\nVariables\n\n\nVariable\nDescription\n\n\n\n\ncrim\nPer capita crime rate by town\n\n\nzn\nProportion of residential land zoned for lots over 25,000 sq. ft.\n\n\nindus\nProportion of non-retail business acres per town\n\n\nchas\nCharles River dummy variable (1 if tract bounds river; 0 otherwise)\n\n\nnox\nNitric oxides concentration (parts per 10 million)\n\n\nrm\nAverage number of rooms per dwelling\n\n\nage\nProportion of owner-occupied units built prior to 1940\n\n\ndis\nWeighted distances to five Boston employment centers\n\n\nrad\nIndex of accessibility to radial highways\n\n\ntax\nFull-value property tax rate per $10,000\n\n\nptratio\nPupil-teacher ratio by town\n\n\nlstat\n% lower status of the population\n\n\nmedv\nMedian value of owner-occupied homes in $1000's",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "01-Linear-regression.html#predict-per-capita-crime-rate-using-best-subset-selection",
    "href": "01-Linear-regression.html#predict-per-capita-crime-rate-using-best-subset-selection",
    "title": "2  Linear regression",
    "section": "2.2 Predict per capita crime rate using best subset selection",
    "text": "2.2 Predict per capita crime rate using best subset selection\nWe will now try to predict the per capita crime rate using best subset selection. We will first split the data into a training set and a test set. We will then fit a separate least squares regression to the training set for each possible combination of the 13 predictors. Then, we will use the regsubsets() function from the leaps library to perform best subset selection.\nBecause there is no predict() method for regsubsets() we have to create the same function ourselves. This function takes in the regsubsets object and the data set, and returns the predictions for the best model.\n\n predict.regsubsets &lt;- function(object, \n                                newdata, \n                                id, ...) {\n  form &lt;- as.formula(object$call[[2]])\n  mat &lt;- model.matrix(form, newdata)\n  coefi &lt;- coef(object, id = id)\n  xvars &lt;- names(coefi)\n  mat[, xvars] %*% coefi\n }\n\nNext, we will try to choose among the models of different sizes using cross-validation. This approach is somewhat involved, as we must perform best subset selection within each of the \\(k\\) training sets.\nFirst, we create a vector that allocates each observation to one of \\(k=10\\) folds, and create a matrix in which we will store the results.\n\nk &lt;- 10\nn &lt;- nrow(Boston)\nset.seed(1)\nfolds &lt;- sample(rep(1:k, length = n))\ncv.errors &lt;- matrix(NA, k, 12,\n    dimnames = list(NULL, paste(1:12)))\n\nNow we use a for loop to perform cross-validation. In the \\(j\\)th fold, the elements of folds that equal j are in the test set, and the remainder are in the training set. We make our predictions for each model size (using our new predict() method), compute the test errors on the appropriate subset, and store them in the appropriate slot in the matrix cv.errors.\n\nfor (j in 1:k) {\n  best.fit &lt;- regsubsets(crim ~ .,\n       data = Boston[folds != j, ],\n       nvmax = 12)\n  for (i in 1:12) {\n    pred &lt;- predict(best.fit, Boston[folds == j, ], id = i)\n    cv.errors[j, i] &lt;-\n         mean((Boston$crim[folds == j] - pred)^2)\n   }\n }\n\nThis has given us a \\(10 \\times 12\\) matrix, of which the \\((j,i)\\)th element corresponds to the test MSE for the \\(j\\)th cross-validation fold for the best \\(i\\)-variable model. I use the apply() function to average over the columns of this matrix in order to obtain a vector for which the \\(i\\)th element is the cross-validation error for the \\(i\\)-variable model.\n\nmean.cv.errors &lt;- apply(cv.errors, 2, mean)\nmean.cv.errors\n\n       1        2        3        4        5        6        7        8 \n46.00617 44.22854 44.41638 43.05242 43.47254 43.27042 43.01362 42.91980 \n       9       10       11       12 \n42.75855 42.75734 42.67672 42.71781 \n\nwhich.min(mean.cv.errors)\n\n11 \n11 \n\nmin(mean.cv.errors)\n\n[1] 42.67672\n\npar(mfrow = c(1, 1))\nplot(mean.cv.errors, \n     type = \"b\")\n\n\n\n\n\n\n\n\n\n# Create a data frame\ndata_df &lt;- data.frame(index = 1:length(mean.cv.errors), \n                      value = mean.cv.errors)\n\n# Index to highlight\nhighlight_index &lt;- which.min(mean.cv.errors)\n\n# Set specific breaks on the x-axis\ncustom_breaks &lt;- c(2, 4, 6, 8, 10, 12)\n\n# Plot the data\ncv.plot &lt;- ggplot(data_df, \n       aes(x = index, \n           y = value)) +\n  geom_line() +\n  geom_point() +\n  scale_color_npg() +\n  theme_bw() + \n  geom_point(data = data.frame(index = highlight_index, value = data_df$value[highlight_index]),\n             aes(x = index, \n                 y = value), \n             color = \"red\", \n             shape = 19, \n             size = 3) +\n  labs(x = \"Predictors\", \n       y = \"Mean CV Errors\") +\n  scale_x_continuous(breaks = custom_breaks) +  # Set custom breaks on the x-axis\n  theme(axis.text = element_text(size = 12))\n\ncv.plot\n\n\n\n\n\n\n\nggsave(\"/Users/gustavaagren/Documents/Statistik/Statistisk inlärning/Assignment/cv.pdf\",\n       dpi = 100, \n       width = 18, \n       height = 12, \n       units = \"cm\")\n\nWe see that cross-validation selects a 11-variable model. Now we cab perform the best subset selection on the full data set in order to obtain the best 11-variable model.\n\nreg.best &lt;- regsubsets(crim ~ ., \n                       data = Boston,\n                       nvmax = 13)\nbestsub &lt;- coef(reg.best, 11)\ncoef(reg.best, 11)\n\n  (Intercept)            zn         indus          chas           nox \n 13.801555544   0.045818028  -0.058345869  -0.828283847 -10.022404078 \n           rm           dis           rad           tax       ptratio \n  0.623650191  -1.008539667   0.612822152  -0.003782956  -0.304784434 \n        lstat          medv \n  0.137698956  -0.220092318 \n\n\n\n# Create model with the 11 best predictors\nmodel &lt;- lm(crim ~ zn + indus + chas + nox + rm  + dis + rad + tax + ptratio + lstat + medv, data = Boston)\n\n# Round the coefficients, SE's, statistics and p-value to 2 decimals\nmodel$coefficients &lt;- round(model$coefficients, 2)\n\n# Present model in a table using gt and round all values to two digits\nmodel %&gt;% \n  summary() %&gt;% \n  tidy() %&gt;% \n  gt() %&gt;% \n  tab_header(\"Model with 11 best predictors\") \n\n\n\n\n\n\n\nModel with 11 best predictors\n\n\nterm\nestimate\nstd.error\nstatistic\np.value\n\n\n\n\n(Intercept)\n13.80\n7.05771079\n1.9553082\n5.111011e-02\n\n\nzn\n0.05\n0.01863204\n2.6835489\n7.528970e-03\n\n\nindus\n-0.06\n0.08355054\n-0.7181282\n4.730177e-01\n\n\nchas\n-0.83\n1.18060343\n-0.7030303\n4.823680e-01\n\n\nnox\n-10.02\n5.10379184\n-1.9632462\n5.017815e-02\n\n\nrm\n0.62\n0.59619914\n1.0399210\n2.988853e-01\n\n\ndis\n-1.01\n0.27108496\n-3.7257693\n2.172029e-04\n\n\nrad\n0.61\n0.08712143\n7.0017217\n8.296249e-12\n\n\ntax\n0.00\n0.00516481\n0.0000000\n1.000000e+00\n\n\nptratio\n-0.30\n0.18556279\n-1.6167034\n1.065807e-01\n\n\nlstat\n0.14\n0.07197165\n1.9452104\n5.231662e-02\n\n\nmedv\n-0.22\n0.05975868\n-3.6814738\n2.574373e-04\n\n\n\n\n\n\n\nUsing the best subset selection, we have been able to reduce ourselves to fewer predictors. The model with the 11 best predictors has an adjusted \\(R^2\\) of 0.63, which is higher than the model with all predictors. Still, it’s not a very efficient model in the sense that it has many variables. More ways to reduce the number of predictors could be to use ridge regression or lasso regression. More about this in the machine learning section.",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "01-Linear-regression.html#references",
    "href": "01-Linear-regression.html#references",
    "title": "2  Linear regression",
    "section": "2.3 References",
    "text": "2.3 References",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>2</span>  <span class='chapter-title'>Linear regression</span>"
    ]
  },
  {
    "objectID": "02-Logit-probit.html",
    "href": "02-Logit-probit.html",
    "title": "3  Logit and probit (and maybe tobit)",
    "section": "",
    "text": "This review will cover the basics of logit and probit models. We will discuss the differences between the two models, how to estimate them, and how to interpret the results. We will also discuss the assumptions of the models and how to test them.",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>3</span>  <span class='chapter-title'>Logit and probit (and maybe tobit)</span>"
    ]
  },
  {
    "objectID": "02-Survival-analysis.html",
    "href": "02-Survival-analysis.html",
    "title": "4  Survival analysis",
    "section": "",
    "text": "4.1 Packages and data\nFor this we need the following packages and data\n# | echo: false\n# | output: false\n\n# Load packages\nif (!require(\"pacman\")) install.packages(\"pacman\")\n\nLoading required package: pacman\n\npacman::p_load(\n  # Data manipulation and importation\n  asaur, # for data set pancreatic\n  tidyverse,\n  purrr,\n  haven,\n  showtext, # fonts\n  \n  # Survival analyis\n  survival,\n  flexsurv,\n  survminer,\n  cmprsk, # competing risks\n  mstate, # multi-state models\n  \n  # Data visualization\n  ggplot2,\n  ggsurvfit,\n  modelsummary,\n  kableExtra,\n  knitr,\n  DT, # tables\n  gt #tables\n)\nThe data used in this review is simulated data from 14,294 prostate cancer patients from Lu-Yao et al. study https://quarto.org (DOI:10.1001/jama.2009.1348). It has the following five variables:\nVariables\n\n\nVariable\nDescription\n\n\n\n\ngrade\nThe grade of prostate cancer (moderately differentiated, poorly differentiated).\n\n\nstage\nThe stage of prostate cancer (T1c if screen-diagnosed using a prostate-specific antigen blood test, T1ab if clinically diagnosed without screening, or T2 if palpable at diagnosis).\n\n\nageGroup\nThe age of diagnosis (e.g., 80+, 75-79, 70-74).\n\n\nsurvTime\nSurvival time in months.\n\n\nstatus\nThe status indicator (0 = censored, 1 = died of prostate cancer, 2 = died of some other cause).\n\n\ndied\nA binary variable indicating whether the patient died or not.",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "02-Survival-analysis.html#kaplan-meier-and-non-parametric-tests",
    "href": "02-Survival-analysis.html#kaplan-meier-and-non-parametric-tests",
    "title": "4  Survival analysis",
    "section": "4.2 Kaplan-Meier and non-parametric tests",
    "text": "4.2 Kaplan-Meier and non-parametric tests\nLet’s start by estimating the Kaplan-Meier survival curve for the prostate cancer patients. The Kaplan-Meier estimator is a non-parametric method used to estimate the survival function from lifetime data. The survival function is defined as the probability that an individual survives from the time of entry into the study until a specified time point. We fit the Kaplan-Meier estimator for the prostate cancer patients and plot the survival curves by age of dyagnosis.\n\n\nWarning: The `size` argument of `element_rect()` is deprecated as of ggplot2 3.4.0.\nℹ Please use the `linewidth` argument instead.\n\n\n\n\n\n\n\n\nFigure 4.1: Kaplan-Meier plot\n\n\n\n\n\nIn figure Figure 4.1 we can see that the survival probability decreases with age of diagnosis.\nAdding confidence intervals is a way to visually assess the uncertainty in the survival estimates.\n\n\n\n\n\n\n\n\nFigure 4.2: Kaplan-Meier plot with confidence intervals\n\n\n\n\n\nFigure Figure 4.2 shows the Kaplan-Meier plot with confidence intervals. From that the survival curves for the two oldest age groups are separated from the other while the two younger groups confidence intervals seem to be overlapping a little.\nWe can also perform the log-rank test and the Wilcoxon test to compare the survival distributions of the two groups.Both of these are non-parametric tests used to compare the survival distributions of two groups.\n\n\n\n\n\n\n\n\nLog-rank and Wilcoxon test\n\n\nTest\nStatistic\nP_value\n\n\n\n\nLog-rank test\n608.27\n0\n\n\nWilcoxon test\n551.28\n0\n\n\n\n\n\n\n\nThe log-rank test and the Wilcoxon test both tell us that there is a significant difference in survival between age groups.",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "02-Survival-analysis.html#cox-proportional-hazards-model",
    "href": "02-Survival-analysis.html#cox-proportional-hazards-model",
    "title": "4  Survival analysis",
    "section": "4.3 Cox proportional hazards model",
    "text": "4.3 Cox proportional hazards model\nAnother way to compare survival between groups is to use a Cox proportional hazards model. The Cox proportional hazards model is a semi-parametric model used to estimate the effect of covariates on survival time. It assumes that the hazard ratio is constant over time.\n\n\n\n\n\n\n\n\nCox Proportional Hazard model 1\n\n\ncoef\nexp(coef)\nse(coef)\nPr(&gt;|z|)\n\n\n\n\n0.199\n1.221\n0.077\n0.01\n\n\n0.563\n1.757\n0.072\n0.00\n\n\n1.084\n2.958\n0.069\n0.00\n\n\n\n\n\n\n\nThe Cox proportional hazards model shows that the hazard ratio is significantly higher for all groups compared with the youngest group (66-69). For the 70-74 age group the hazard is 1.2 times higher. For 75-79 age group the hazard is 1.75 times higher. For the oldest group (80+) the hazard is almost 3 times higher. When using estimating a Cox proportional hazards model one can also use other covariates to estimate the hazard ratio.\n\n\n\n\n\n\n\n\nCox Proportional Hazard model 2\n\n\ncoef\nexp(coef)\nse(coef)\nPr(&gt;|z|)\n\n\n\n\n0.199\n1.220\n0.078\n0.01\n\n\n0.555\n1.743\n0.072\n0.00\n\n\n1.029\n2.798\n0.070\n0.00\n\n\n0.471\n1.602\n0.035\n0.00\n\n\n-0.453\n0.636\n0.042\n0.00\n\n\n-0.156\n0.856\n0.037\n0.00\n\n\n\n\n\n\n\nIn this model we’ve added the grade and stage covariates. from the results we see that the hazard ratio for patients with poorely differentiated cancer is 1.6 times higher than for patients with moderately differentiated cancer. The hazard ratio for patients with stage T1c and T2 cancer is lower than for those with stage T1ab cancer.\nWe can also take a look at the partial residuals plot of each of the covariates in the model. The partial residuals plot shows the effect of each covariate on the survival function while adjusting for the other covariates in the model.\n\n\n\n\n\n\n\n\nFigure 4.3: Partial residuals plot 1\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.4: Partial residuals plot 2\n\n\n\n\n\n\n\n\n\n\n\n\n\nFigure 4.5: Partial residuals plot 3\n\n\n\n\n\n\n4.3.1 Cox proportional hazards model assumptions\nNext step in our analysis is to assess the validity of the model by using diagnostic plots in order to understand whether the underlying assumptions needed for this model are met. It is also a way to understand whether this model is appropriate for our data at hand. One of the assumption of a Cox proportional hazards regression model is that the hazards are proportional at each point in time throughout follow-up and this is what we will try to investigate.\nFirst we look at the log(-log(survival)) plot to check for deviations from the proportional hazards assumption in the first Cox model we made.\n\n\n\n\n\n\n\n\nFigure 4.6: Deviations from the proportional hazards assumption (sex)\n\n\n\n\n\nFigure 4.6 shows that the lines are fairly but not perfectly parallel. This indicates that the proportional hazards assumption may hold for age group.\n\n\n\n\n\n\n\n\nFigure 4.7: Deviations from the proportional hazards assumption (sex)\n\n\n\n\n\nFigure 4.7 shows that the lines are parallel all the time. This indicates that the proportional hazards assumption may hold for age group.\n\n\n\n\n\n\n\n\nFigure 4.8: Deviations from the proportional hazards assumption (sex)\n\n\n\n\n\nFigure 4.8 shows that the lines are fairly but not perfectly parallel. This indicates that the proportional hazards assumption may hold for age group.\nWe now do a hypothesis test of whether the effect of each covariate differs according to time, and a global test of all covariates at once. This is done by testing for an interaction effect between the covariate and log(time). A significant p-value indicates that the proportional hazards assumption is violated. This is presented in the table below.\n\n\n\n\n\n\n\n\nSchoenfeld test for proportional hazards assumption\n\n\nvariable\nchisq\ndf\np.value\n\n\n\n\nageGroup\n8.900394\n3\n0.0306449435\n\n\ngrade\n3.639644\n1\n0.0564189807\n\n\nstage\n10.383688\n2\n0.0055617419\n\n\nGLOBAL\n25.536440\n6\n0.0002715458\n\n\n\n\n\nTest of proportional hazard assumption\n\n\nFrom the test we can see that age group has a p-value &lt; 0.05 which indicates that the proportional hazards assumption may be violated. For grade the null of no violation of the proportional hazards assumption cannot be rejected. Stage has a p-value &lt; 0.05 which indicates that the proportional hazards assumption may be violated. The global test for all covariates also has a p-value &lt; 0.05 which indicates that the proportional hazards assumption is violated and that the null of no violation of the proportional hazards assumption should be rejected.\nTo investigate further we want to look at the plots visualizing the scaled Schoenfeld residuals over time.\n\n\n\n\n\n\n\n\nFigure 4.9: Schoenfeld residuals over time (age group)\n\n\n\n\n\nPlots of the Schoenfeld residuals Deviation from a zero-slope line is evidence that the proportional hazards assumption is violated. One should note though that the violation of the proportionality assumption doesn’t necessarily mean the model is invalid since the violation might only be present for some subset of the population. In Figure 4.9 we see that the line is not straight (it looks as if it has a polynomial shape) which implies non-proportionality in the form of a rising hazard ratio over time.\n\n\n\n\n\n\n\n\nFigure 4.10: Schoenfeld residuals over time (grade)\n\n\n\n\n\nIn Figure 4.10 the line is not straight implying non-proportionality in the form of a declining hazard ratio over time.\n\n\n\n\n\n\n\n\nFigure 4.11: Schoenfeld residuals over time (stage)\n\n\n\n\n\nIn Figure 4.11 the line is not perfectly straight but close to although it has a somewhat declining hazard ratio over time.",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "02-Survival-analysis.html#competing-risks-analysis",
    "href": "02-Survival-analysis.html#competing-risks-analysis",
    "title": "4  Survival analysis",
    "section": "4.4 Competing risks analysis",
    "text": "4.4 Competing risks analysis\nWe know that the risk of death increases for a patient with a poor grade for both cancer and other causes of death. But does this effect differ? Using competing risk analysis we can determine whether there is a difference and what the difference in the effect the grade of the cancer has on the hazard of death from prostate cancer and other causes of death.\nWe will do this using the method by Putter et al. (reference) which is implemented in the comprisk package in R. This begins by setting up a transition matrix.\n\n\n            to\nfrom         event-free prostate other\n  event-free         NA        1     2\n  prostate           NA       NA    NA\n  other              NA       NA    NA\n\n\nUsing the function msprep we can create a new data set that is suitable for competing risk analysis. This new data set, its 10 first rows seen in the table below, has twice as many rows as the original data. The first column, id, refering to the patient number is repeated twice. The column trans indicates the cause of death; “1” refers to death from prostate cancer and “2” refers to death from other causes. The column time is the time to the event or censoring, and the column status indicates whether the event was observed or censored. The columns grade, ageGroup, and stage are repeated for each cause of death. .\n\n\n\n\n\n\n\n\nFirst ten rows of the new data\n\n\nid\nfrom\nto\ntrans\nTstart\nTstop\ntime\nstatus\ngrade\nageGroup\nstage\n\n\n\n\n1\n1\n2\n1\n0\n18\n18\n0\nmode\n80+\nT1c\n\n\n1\n1\n3\n2\n0\n18\n18\n0\nmode\n80+\nT1c\n\n\n2\n1\n2\n1\n0\n23\n23\n0\nmode\n75-79\nT1ab\n\n\n2\n1\n3\n2\n0\n23\n23\n0\nmode\n75-79\nT1ab\n\n\n3\n1\n2\n1\n0\n37\n37\n0\npoor\n75-79\nT1c\n\n\n3\n1\n3\n2\n0\n37\n37\n0\npoor\n75-79\nT1c\n\n\n4\n1\n2\n1\n0\n27\n27\n0\nmode\n70-74\nT2\n\n\n4\n1\n3\n2\n0\n27\n27\n0\nmode\n70-74\nT2\n\n\n5\n1\n2\n1\n0\n42\n42\n0\nmode\n70-74\nT1c\n\n\n5\n1\n3\n2\n0\n42\n42\n0\nmode\n70-74\nT1c\n\n\n\nThe dataset set contains 28588 rows, twice as many as the original data\n\n\n\n\n\n\n\n\nWe will now be able to look at the hazard of death from prostate cancer while treating the other causes as censored and vice versa. We can now obtain estimates of the effects of our variables of interest on prostate-specific and other death causes.\nWe begin with looking at the effect of grade, age group, and stage on the hazard of death from prostate cancer.\n\n\n\n\n\n\n\n\nCompeting risk analysis 1, other causes of death treated as censored\n\n\nVariable\nCoefficient\nexp.coef.\nStd..Error\nP.Value\n\n\n\n\ngradepoor\n1.422\n4.146\n0.072\n0.000\n\n\nageGroup70-74\n0.182\n1.199\n0.202\n0.369\n\n\nageGroup75-79\n0.822\n2.275\n0.184\n0.000\n\n\nageGroup80+\n1.219\n3.385\n0.179\n0.000\n\n\nstageT1c\n-0.280\n0.756\n0.102\n0.006\n\n\nstageT2\n0.128\n1.137\n0.089\n0.150\n\n\n\n\n\n\n\nThe table shows the estimates of the effect of grade, age group, and stage on the hazard of death from prostate cancer. The hazard of death from prostate cancer increases for patients with a poor grade.\nNow, let’s look at the effect of grade, age group, and stage on the hazard of death from other causes.\n\n\n\n\n\n\n\n\nCompeting risk analysis 2, prostate cancer treated as censored\n\n\nVariable\nCoefficient\nexp.coef.\nStd..Error\nP.Value\n\n\n\n\ngradepoor\n0.187\n1.206\n0.041\n0.000\n\n\nageGroup70-74\n0.204\n1.226\n0.084\n0.015\n\n\nageGroup75-79\n0.501\n1.651\n0.079\n0.000\n\n\nageGroup80+\n0.993\n2.699\n0.076\n0.000\n\n\nstageT1c\n-0.485\n0.615\n0.046\n0.000\n\n\nstageT2\n-0.220\n0.802\n0.042\n0.000\n\n\n\n\n\n\n\nIn this table, we see the estimates of the effect of grade, age group, and stage on the hazard of death from other causes. The hazard of death from other causes increases for patients with a poor grade but the exp coef is much smaller than that of prostate cancer. The question is then, is there a difference in this effect? Let’s test this using Putter et al.’s method.\n\n\n\n\n\n\n\n\nCompeting risk analysis 3, using Putter et al.'s method\n\n\nVariable\nCoefficient\nexp.coef.\nStd..Error\nP.Value\n\n\n\n\ngradepoor\n2.748\n15.616\n0.148\n0.00\n\n\ntrans\nNA\nNA\n0.000\nNA\n\n\nageGroup70-74\n0.199\n1.220\n0.078\n0.01\n\n\nageGroup75-79\n0.555\n1.742\n0.072\n0.00\n\n\nageGroup80+\n1.028\n2.795\n0.070\n0.00\n\n\nstageT1c\n-0.452\n0.636\n0.042\n0.00\n\n\nstageT2\n-0.156\n0.856\n0.037\n0.00\n\n\ngradepoor:trans\n-1.285\n0.277\n0.082\n0.00\n\n\n\n\n\n\n\nThis last table shows the estimates of the competing risk analysis using Putter et al.’s method. Of main interest is to look at the interaction between grade poor and trans. This interaction term estimates the difference between the effect on prostate cancer death and death from other causes. The estimate is highly significant and represents the additional effect poor grade has on the risk of death from other causes relative to its effect on the risk of prostate cancer death. The hazard of death fro other causes is is 0.27 times the hazard of death from prostate cancer.\nTo conclude, this analysis has shown that a poor grade of prostate cancer strongly affects the risk of dying from prostate cancer. This effect is also much stronger on the risk of dying from prostate cancer than from other causes. The analysis also shows that the risk of dying from other causes is higher for older patients and for patients with more advanced stages of prostate cancer.",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>4</span>  <span class='chapter-title'>Survival analysis</span>"
    ]
  },
  {
    "objectID": "03-Selection-correction.html",
    "href": "03-Selection-correction.html",
    "title": "5  Selection correction",
    "section": "",
    "text": "This review will cover:\n\nHeckman selection model\nOaxaca-Blinder decomposition",
    "crumbs": [
      "Old-school econometrics",
      "<span class='chapter-number'>5</span>  <span class='chapter-title'>Selection correction</span>"
    ]
  },
  {
    "objectID": "Modern-econometrics.html",
    "href": "Modern-econometrics.html",
    "title": "Modern econometrics",
    "section": "",
    "text": "This section will cover the methods within the causal inference framework.\nThe topics covered in this section are:\n\nDifference-in-differences\nRegression discontinuity design\nInstrumental variables\nSynthetic control",
    "crumbs": [
      "Modern econometrics"
    ]
  },
  {
    "objectID": "04-Diff-n-diff.html",
    "href": "04-Diff-n-diff.html",
    "title": "6  Difference-in-differences",
    "section": "",
    "text": "Difference-in-differences (DiD) is a method used to estimate the causal effect of a treatment or intervention on a treatment group compared to a control group. The method is based on the assumption that the treatment and control groups would have followed the same trend over time in the absence of the treatment. The DiD method compares the difference in outcomes between the treatment and control groups before and after the treatment is implemented to estimate the causal effect of the treatment.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>6</span>  <span class='chapter-title'>Difference-in-differences</span>"
    ]
  },
  {
    "objectID": "05-RD.html",
    "href": "05-RD.html",
    "title": "7  Regression Discontinuity",
    "section": "",
    "text": "Regression Discontinuity (RD) is a quasi-experimental design that is used to estimate the causal effect of a treatment or intervention. The design is based on the idea that units that are just above or below a threshold are similar in all respects except for the treatment. The treatment is assigned based on whether the unit’s value of a continuous variable is above or below a threshold. The RD design is used when random assignment is not possible, but there is a clear threshold that determines whether a unit receives the treatment or not.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>7</span>  <span class='chapter-title'>Regression Discontinuity</span>"
    ]
  },
  {
    "objectID": "06-IV.html",
    "href": "06-IV.html",
    "title": "8  Instrumental Variable",
    "section": "",
    "text": "Instrumental variables (IV) are used in econometrics to estimate causal relationships when the assumptions of other methods such as ordinary least squares regression are violated. The IV method is used when the independent variable is correlated with the error term in a regression model. This correlation can lead to biased estimates of the coefficients. The IV method is used to remove this bias by finding an instrument that is correlated with the independent variable but not with the error term.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>8</span>  <span class='chapter-title'>Instrumental Variable</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html",
    "href": "07-Synthetic-control.html",
    "title": "9  Synthetic Control",
    "section": "",
    "text": "9.1 Exit Interview\nSet up a dedicated time to meet with Gavin and/or your main collaborator (for undergraduate researchers) to talk about your time in the lab, and to go through the below checklist to make sure these have been done. Besides the checklist, things to talk about include the best part of being in our team, whether you got the support you needed and what could we improve for mentoring and training someone in your role in the future. Checklist for students, staff, and post docs",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#project-documentation",
    "href": "07-Synthetic-control.html#project-documentation",
    "title": "9  Synthetic Control",
    "section": "9.2 Project Documentation",
    "text": "9.2 Project Documentation\nProject work should be hosted either in a github repository linked to from the faylab account, or saved on the lab_fay server drive (prefereably both).\nEach project should have an easily found README text file that provides information for others so they can navigate and use your work, and give contact information for authors (and any data creators/use restrictions if propietary data). Ideally, the README should also include links to publications and presentations from the work.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#publications-and-presentations",
    "href": "07-Synthetic-control.html#publications-and-presentations",
    "title": "9  Synthetic Control",
    "section": "9.3 Publications and Presentations",
    "text": "9.3 Publications and Presentations\nEnsure that publications and presentations from your projects are archived in the appropriate folder on the lab Google Drive.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#data",
    "href": "07-Synthetic-control.html#data",
    "title": "9  Synthetic Control",
    "section": "9.4 Data",
    "text": "9.4 Data\nData used in support of your projects should be:\n\nSaved in appropriate, non-proprietary format with accompanying metadata\nEither in a public archive (e.g., the github repo or another public archive), or if data is proprietary, a ‘snapshot’ version of the data used in the project should be saved in a private repository accessible to lab members. Alternatively, data could be saved on the lab_fay server drive with appropriate documentation.\nLinked and briefly described in the project README.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#code",
    "href": "07-Synthetic-control.html#code",
    "title": "9  Synthetic Control",
    "section": "9.5 Code",
    "text": "9.5 Code\nCode used or developed for the project should be:\n\ncomplete and well-documented, including information in a README about what each file does and workflow to run the code\nSaved on the lab_fay section of the storage server, and also preferably available on github in a public repository.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#timelines-for-publications",
    "href": "07-Synthetic-control.html#timelines-for-publications",
    "title": "9  Synthetic Control",
    "section": "9.6 Timelines for publications",
    "text": "9.6 Timelines for publications\nScience is not finished until it has been communicated. Hopefully, you will publish your results in the peer-reviewed literature so that others can learn and build from your work. During your exit interview, make a plan with Gavin for remaining publications, including a timeline for submission.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#turning-in-equipment",
    "href": "07-Synthetic-control.html#turning-in-equipment",
    "title": "9  Synthetic Control",
    "section": "9.7 Turning in equipment",
    "text": "9.7 Turning in equipment\nEnsure any lab equipment (e.g. computer and peripherals) you have been using has been returned to the lab, office furniture is present and accounted for, and that tagged equipment has been logged in with Ashleigh. Make sure any problems with equipment are documented and that Gavin and Ashleigh have been made aware of them.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#lab-library",
    "href": "07-Synthetic-control.html#lab-library",
    "title": "9  Synthetic Control",
    "section": "9.8 Lab library",
    "text": "9.8 Lab library\nReturn all books checked out from the lab library.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "07-Synthetic-control.html#removing-key-card-access",
    "href": "07-Synthetic-control.html#removing-key-card-access",
    "title": "9  Synthetic Control",
    "section": "9.9 Removing key card access",
    "text": "9.9 Removing key card access\nKey card access to the SMAST facilities will be terminated at the end of your contract. During your exit interview, discuss with Gavin whether modifications need to be made to this timeline.",
    "crumbs": [
      "Modern econometrics",
      "<span class='chapter-number'>9</span>  <span class='chapter-title'>Synthetic Control</span>"
    ]
  },
  {
    "objectID": "Machine-learning.html",
    "href": "Machine-learning.html",
    "title": "Machine Learning",
    "section": "",
    "text": "Contents",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "Machine-learning.html#contents",
    "href": "Machine-learning.html#contents",
    "title": "Machine Learning",
    "section": "",
    "text": "Regularisation (Ridge and Lasso)\nDecision Tree based methods\nSupport Vector MAchines",
    "crumbs": [
      "Machine Learning"
    ]
  },
  {
    "objectID": "08-Regularisation.html",
    "href": "08-Regularisation.html",
    "title": "10  Regularisation using Ridge and Lasso",
    "section": "",
    "text": "Gavin does not typically accept students into the lab without being able to guarantee at least 2 years of full funding.\nFunded position openings in the lab will be posted through the lab website, twitter account, and via other distribution feeds (e.g. EcoLog).\nGavin encourages all students to apply for external funding through graduate scholarships. Some relevant ones include the NMFS-Sea Grant Graduate Fellowships in Population and Ecosystem Dynamics, NSF Graduate Research Fellowship Program, Ford Foundation, NOAA Nancy Foster scholarships, and Margaret A. Davidsonn Graduate Fellowships, though there are many other opportunities (more provided in the link below).\nHere is Gavin’s living document of recurring funding opporunities Google Doc. Gavin not only encourages students to obtain additional funding, but also staff and post-docs. And for all to apply for travel grants when attending conferences, where applicable.\nThis resource is updated fairly regularly, and largely contains resources to fund yourself/research. It includes the opportunity name, institituion, link, grad/undergrad/postdoc/all/early career/etc designation, the deadline, and important notes.\nGavin will also share funding opportunites as they come up, either to the group or in individual meetings.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>10</span>  <span class='chapter-title'>Regularisation using Ridge and Lasso</span>"
    ]
  },
  {
    "objectID": "09-Tree-based.html",
    "href": "09-Tree-based.html",
    "title": "11  Tree-based models",
    "section": "",
    "text": "11.1 Inclusive science communication\nThis Google Doc resource created and curated by Dr. Sunshine Menezes, the ED of the Metcalf Institute at URI, covers a wide range of topics related to inclusive science communication, education, and public engagement with STEM topics. We suggest you take a look!",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tree-based models</span>"
    ]
  },
  {
    "objectID": "09-Tree-based.html#curriculum-vitae",
    "href": "09-Tree-based.html#curriculum-vitae",
    "title": "11  Tree-based models",
    "section": "11.2 Curriculum Vitae",
    "text": "11.2 Curriculum Vitae\nKeep your CV up to date. We share our CVs within the group in this Google Drive folder, both to give each other ideas about what to include, but also having these in one place makes it easier for Gavin and others to reference information from them when needed.\nHere is a link to a lab-chat issue on CV building based on a 05/05/2020 lab meeting.\n\n11.2.1 Google Scholar profiles\nHaving a Google Scholar profile is a very easy way to curate your web presence and have a place where people can find you and your published papers and reports.\nWe recommend everyone sets up a Google Scholar profile as soon as they have a paper or technical report. It takes about 5-10 minutes to set up, and one advantage is that Google Scholar self-updates and will add your papers and citations as you publish and people use your work. This is a really easy step to creating a web presence without having to set up a website.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tree-based models</span>"
    ]
  },
  {
    "objectID": "09-Tree-based.html#personal-websites",
    "href": "09-Tree-based.html#personal-websites",
    "title": "11  Tree-based models",
    "section": "11.3 Personal Websites",
    "text": "11.3 Personal Websites\nWhy have your own website?\n\nPeople will Google you. They might as well see what you want them to.\nGive people an easy way to find you and your work.\n\nOne way to do this is a personal website. This doesn’t have to be extensive, but can be a good way to curate your web presence.\nThere are heaps of tools out there to create and publish websites. Our recommendation is to choose something that it is easy to update, and fun / interesting for you to use. Many options are free.\nSome options for creating sites include: GoogleSites, Weebly, Squarespace, WordPress, R Markdown, GitHub.\nWhat should you include?\nUltimately this is up to you and how you would like to present yourself and your work. But some common things you could include are: not an exhaustive list\n\nBioSketch\nResearch interests/overview\nProjects\nPublications\nCV\nBlog posts\nContact info\n\nAn example website (though many amazing others exist and you should check those out): Ashleigh’s webpage using the free version of Weebly",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tree-based models</span>"
    ]
  },
  {
    "objectID": "09-Tree-based.html#twitter",
    "href": "09-Tree-based.html#twitter",
    "title": "11  Tree-based models",
    "section": "11.4 Twitter",
    "text": "11.4 Twitter\nTwitter is a great tool for interacting with others about your science. It is also a great platform for learning about new research and papers, finding out about jobs and fellowships, learning and discussing methods, discussing topics about how we do science, professional development, mental health and expectations around science and graduate school, and is a great venue for bringing ones whole self to the science. etc. Most of us have our own personal Twitter accounts and use them to varying degress. Everyone in the lab can post to the lab Twitter account too. Ask Ashleigh if you are interested in doing a takeover of thefaylab account for a week (which could also be a chance to dip your toe in to Twitter if you don’t use it).\n[link to paper on use of twitter for rstats!]",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tree-based models</span>"
    ]
  },
  {
    "objectID": "09-Tree-based.html#social-media",
    "href": "09-Tree-based.html#social-media",
    "title": "11  Tree-based models",
    "section": "11.5 Social Media",
    "text": "11.5 Social Media\n[Link to papers on social media in fisheries ]\nAs you know, there are many social media platforms, and many (all?) of these are being used in some way for science communication. This includes but not limited to:\n\nTwitter\nInstagram\nFacebook\nLinkedIn\nTikTok\nYoutube\nResearchgate\n\nAll have a different purpose and type of engagement. Think deliberately about how (and if) you want to use these tools. Using social media as part of your science communication portfolio can be useful and is popular, but it is not a requirement to be a successful scientist.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tree-based models</span>"
    ]
  },
  {
    "objectID": "09-Tree-based.html#contributing-to-regional-publications",
    "href": "09-Tree-based.html#contributing-to-regional-publications",
    "title": "11  Tree-based models",
    "section": "11.6 Contributing to regional publications",
    "text": "11.6 Contributing to regional publications\n[more to come on engaging with our local press, etc.]",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>11</span>  <span class='chapter-title'>Tree-based models</span>"
    ]
  },
  {
    "objectID": "10-SVM.html",
    "href": "10-SVM.html",
    "title": "12  Support Vector Machines",
    "section": "",
    "text": "12.1 MS students\nThesis Proposal: MS students in the Fay lab prepare a thesis proposal during their first year to identify and refine their thesis research topic ideas and outline methods that will be used to address the research questions. The proposal also serves as a guiding document for your initial committee meeting(s). Scope and plans for the MS thesis research will be developed through your meetings with Gavin and with the project team (for students whose work is being funded by a research grant). Your thesis proposal should contain a literature review, identification of research questions, an overview of the methods that will be used, and description of expected results and relevance/significance of the research. Aim for the proposal to be about 8 pages of text. While not a formal requirement for the IMS program, a MS thesis proposal serves to clarify and outline expectations for the degree for both the student and the thesis committee, and serves as a blueprint for building out the thesis itself (indeed you may likely re-use some of the text later). Thesis proposals written by other lab members are good examples of how these documents can be structured.\nThesis Committee: Forming your thesis committee: Your MS thesis committee serves as your guidance team during your research, and should be viewed as colleagues and collaborators. While building your proposal, discuss with Gavin who should be on the committee. Three committee members is the usual size, though sometimes students will have a fourth member. Generally in our program a committee will be made up of your advisor (Gavin), an additional UMassD faculty member, and an external member (often an agency collaborator on the project, e.g. at NOAA NMFS). See the official university guidelines for more specifics [link here].\nMS Thesis: [guidance on thesis structure to come]\nRemember, not everything will go to plan and your interests and findings may change. Content of theis proposals is not set in stone and can always be revised with communication and collaboration with the committee. View proposals as items that help you build your scientific products rather than evaluative procedures that need to be checked off.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  },
  {
    "objectID": "10-SVM.html#ph.d.-students",
    "href": "10-SVM.html#ph.d.-students",
    "title": "12  Support Vector Machines",
    "section": "12.2 Ph.D. students",
    "text": "12.2 Ph.D. students\nPhD dissertations in the Fay lab tend to comprise chapters that will form ~4 publications. Gavin encourages students to think about dissertation chapters as publishable units. In the disseration, these 4 chapters will be accompanied by a general Introduction, and a concluding Synthesis chapter that ties the work presented in the dissertation together under the overarching research theme and goals.\nMilestones\n\n12.2.1 Pre-proposal (or “Preposal”, copyright M. Winton)\nPhD students in the Fay lab prepare a pre-proposal during their first year to outline the overall goals and themes of the dissertation research topic, and identify the individual research chapter topic questions. The pre-proposal serves as a guiding document and conversation starter for your initial committee meeting. Scope and plans for the thesis research will be developed through your meetings with Gavin and with the project team (for students whose work is being funded by a research grant). Your pre-proposal should contain a literature review, identification of research questions and potential chapter units, an overview of the methods that will be used to address the research questions, proposed course of study (coursework, etc.), and relevance/significance of the research. Aim for the pre-proposal to be about 5 pages of text. A 2-page precis version is useful when initially talking with committee members. While not a formal requirement for the IMS program, a pre-proposal serves to clarify student interests and goals for the degree for both the student and the thesis committee, and serves as a blueprint for building out the preliminary work and the dissertation proposal (indeed you may likely re-use some of the text later).\n\n\n12.2.2 Dissertation Committee\nYour PhD thesis committee serves as your guidance team during your research, and should be viewed as colleagues and collaborators. There is a required minimum for interaction with the committee, but you should add to this. After you have formed the committee, it is advisable to hold a committee meeting at least once a year, but it is encouraged to continuingly interact with committee members.\nWhile building your pre-proposal, discuss with Gavin who should be on the committee. For PhD committees, four members is the usual size, though often students will have a fifth member. Generally in our program a committee will be made up of your advisor (Gavin), two additional UMassD faculty members, and 1-2 external members (e.g. agency collaborators on the project funding the research, e.g. collaborators at NOAA NMFS). Aim to hold your first committee meeting no later than during the 3rd semester of your program. See the official university guidelines for more specifics [link here].\n[something on what is expected at committee meetings, e.g. presentation]\n\n\n12.2.3 Comprehensive Exams\nAs detailed in the IMS policy, PhD students undertake a two-part comprehensive examination, a 2-day written exam, and a public presentation and defense of the proposal. The written exam is taken once students have completed their coursework requirements (say after the 4th or before/during the 5th semester). Questions on the written exam include both open and closed book sections and will cover coursework and questions related to your field and dissertation topics. The proposal defense is usually scheduled the semester following successful completion of the written exam. Full requirement details for the exams are in the IMS policy. The comprehensive exam will be planned in coordination with Gavin, and the written exam requires 1 month preparation time (1 month of full-time work). See guidance below. [Written exam format & guidance] [Proposal defense format & guidance]\n\n\n12.2.4 Dissertation Proposal\nA proposal describing the research that will comprise the dissertation is a required, formal part of the academic program. Proposals should describe the goals and themes of the research to be conducted, and its context within current state of knowledge. The proposal should outline the research questions that each chapter will address, and give a description of the methods that will be used in each. The proposal serves as a guiding document for the remainder of your program and is the document that you and your committee will use to agree on the scope of work to be completed to fulfill the degree requirements. Your thesis proposal should contain a literature review, identification of research questions, an overview of the methods that will be used for each of these, and description of expected results and relevance/significance of the research. Aim for the proposal to be about 15 pages of text. Detail of planning for each chapter will not be as comprehensive (it is expected that later chapters may not be as fleshed out), but the proposal should provide enough detail of what the student will do for the committee to evaluate whether the scope of work is sufficient (and likely make recommendations for reducing or refining this). It is common for students to have completed or be close to having completed work for at least one of the chapters by the time of defending the proposal.\nUltimately, the proposal serves to clarify and outline expectations for the degree for both the student and the thesis committee, and serves as a blueprint for building out the thesis itself (indeed you will re-use some of the text later).\nPhD Thesis: [guidance on thesis structure to come]\nRemember, not everything will go to plan and your interests and findings may change. Content of thesis proposals is not set in stone and can always be revised with communication and collaboration with the committee. View proposals and exams as items that help you build your scientific products rather than evaluative procedures that need to be checked off.",
    "crumbs": [
      "Machine Learning",
      "<span class='chapter-number'>12</span>  <span class='chapter-title'>Support Vector Machines</span>"
    ]
  }
]